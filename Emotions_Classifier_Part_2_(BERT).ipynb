{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7563141,"sourceType":"datasetVersion","datasetId":4403839}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T02:39:08.924522Z","iopub.execute_input":"2024-04-03T02:39:08.924858Z","iopub.status.idle":"2024-04-03T02:39:10.180749Z","shell.execute_reply.started":"2024-04-03T02:39:08.924832Z","shell.execute_reply":"2024-04-03T02:39:10.179762Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/emotions/text.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification, AdamW\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import Adam\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:10.182662Z","iopub.execute_input":"2024-04-03T02:39:10.183102Z","iopub.status.idle":"2024-04-03T02:39:20.482519Z","shell.execute_reply.started":"2024-04-03T02:39:10.183070Z","shell.execute_reply":"2024-04-03T02:39:20.481738Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:20.483652Z","iopub.execute_input":"2024-04-03T02:39:20.484118Z","iopub.status.idle":"2024-04-03T02:39:20.545710Z","shell.execute_reply.started":"2024-04-03T02:39:20.484090Z","shell.execute_reply":"2024-04-03T02:39:20.544785Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/emotions/text.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:20.548476Z","iopub.execute_input":"2024-04-03T02:39:20.549163Z","iopub.status.idle":"2024-04-03T02:39:21.724702Z","shell.execute_reply.started":"2024-04-03T02:39:20.549124Z","shell.execute_reply":"2024-04-03T02:39:21.723884Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:21.725908Z","iopub.execute_input":"2024-04-03T02:39:21.726313Z","iopub.status.idle":"2024-04-03T02:39:21.741740Z","shell.execute_reply.started":"2024-04-03T02:39:21.726283Z","shell.execute_reply":"2024-04-03T02:39:21.740736Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text  label\n0           0      i just feel really helpless and heavy hearted      4\n1           1  ive enjoyed being able to slouch about relax a...      0\n2           2  i gave up my internship with the dmrg and am f...      4\n3           3                         i dont know i feel so lost      0\n4           4  i am a kindergarten teacher and i am thoroughl...      4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>i just feel really helpless and heavy hearted</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ive enjoyed being able to slouch about relax a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>i gave up my internship with the dmrg and am f...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>i dont know i feel so lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>i am a kindergarten teacher and i am thoroughl...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:21.742927Z","iopub.execute_input":"2024-04-03T02:39:21.743239Z","iopub.status.idle":"2024-04-03T02:39:21.764704Z","shell.execute_reply.started":"2024-04-03T02:39:21.743214Z","shell.execute_reply":"2024-04-03T02:39:21.764068Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:21.765664Z","iopub.execute_input":"2024-04-03T02:39:21.765922Z","iopub.status.idle":"2024-04-03T02:39:21.840181Z","shell.execute_reply.started":"2024-04-03T02:39:21.765900Z","shell.execute_reply":"2024-04-03T02:39:21.838845Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 416809 entries, 0 to 416808\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    416809 non-null  object\n 1   label   416809 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 6.4+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:21.841827Z","iopub.execute_input":"2024-04-03T02:39:21.842161Z","iopub.status.idle":"2024-04-03T02:39:22.064942Z","shell.execute_reply.started":"2024-04-03T02:39:21.842132Z","shell.execute_reply":"2024-04-03T02:39:22.063821Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"686"},"metadata":{}}]},{"cell_type":"code","source":"df = df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:22.067925Z","iopub.execute_input":"2024-04-03T02:39:22.068295Z","iopub.status.idle":"2024-04-03T02:39:22.293247Z","shell.execute_reply.started":"2024-04-03T02:39:22.068269Z","shell.execute_reply":"2024-04-03T02:39:22.292386Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:22.297794Z","iopub.execute_input":"2024-04-03T02:39:22.298103Z","iopub.status.idle":"2024-04-03T02:39:22.309753Z","shell.execute_reply.started":"2024-04-03T02:39:22.298073Z","shell.execute_reply":"2024-04-03T02:39:22.308691Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0      i just feel really helpless and heavy hearted      4\n1  ive enjoyed being able to slouch about relax a...      0\n2  i gave up my internship with the dmrg and am f...      4\n3                         i dont know i feel so lost      0\n4  i am a kindergarten teacher and i am thoroughl...      4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i just feel really helpless and heavy hearted</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ive enjoyed being able to slouch about relax a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i gave up my internship with the dmrg and am f...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i dont know i feel so lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am a kindergarten teacher and i am thoroughl...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Check the distribution of labels\nlabel_counts = df['label'].value_counts()\nlabel_counts","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:22.311174Z","iopub.execute_input":"2024-04-03T02:39:22.311573Z","iopub.status.idle":"2024-04-03T02:39:22.324730Z","shell.execute_reply.started":"2024-04-03T02:39:22.311536Z","shell.execute_reply":"2024-04-03T02:39:22.323777Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"label\n1    140779\n0    120989\n3     57235\n4     47664\n2     34497\n5     14959\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Perform stratified sampling to create a balanced dataset\n#balanced_df = df.groupby('label', group_keys=False).apply(lambda x: x.sample(10000))\n\n# Split the balanced dataset into train, validation, and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=50)\nval_df, test_df = train_test_split(test_df, test_size=0.5, stratify=test_df['label'], random_state=50)\n\n# Check the distribution of labels in the balanced dataset\nprint(train_df['label'].value_counts())\nprint(val_df['label'].value_counts())\nprint(test_df['label'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:22.326873Z","iopub.execute_input":"2024-04-03T02:39:22.327424Z","iopub.status.idle":"2024-04-03T02:39:22.572971Z","shell.execute_reply.started":"2024-04-03T02:39:22.327390Z","shell.execute_reply":"2024-04-03T02:39:22.572072Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"label\n1    112623\n0     96791\n3     45788\n4     38131\n2     27598\n5     11967\nName: count, dtype: int64\nlabel\n1    14078\n0    12099\n3     5723\n4     4766\n2     3450\n5     1496\nName: count, dtype: int64\nlabel\n1    14078\n0    12099\n3     5724\n4     4767\n2     3449\n5     1496\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:22.574234Z","iopub.execute_input":"2024-04-03T02:39:22.574548Z","iopub.status.idle":"2024-04-03T02:39:25.948501Z","shell.execute_reply.started":"2024-04-03T02:39:22.574523Z","shell.execute_reply":"2024-04-03T02:39:25.947575Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333fbfa0416c4deaa2d50e318029a4b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c11031ec82459da509882e8be6f89a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5749823811734d35a9681b08b381fa3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dda7040fd91644dda43d021cc494c1b8"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenize text data and encode labels\ndef preprocess_text(text):\n    return tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=64, \n        padding='max_length',\n        truncation=True,\n        return_token_type_ids=False,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:25.949590Z","iopub.execute_input":"2024-04-03T02:39:25.949848Z","iopub.status.idle":"2024-04-03T02:39:25.955728Z","shell.execute_reply.started":"2024-04-03T02:39:25.949825Z","shell.execute_reply":"2024-04-03T02:39:25.954614Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# apply preprocessing\ntrain_df['input_ids'] = train_df['text'].apply(lambda x: preprocess_text(x)['input_ids'])\ntrain_df['attention_mask'] = train_df['text'].apply(lambda x: preprocess_text(x)['attention_mask'])\n\nval_df['input_ids'] = val_df['text'].apply(lambda x: preprocess_text(x)['input_ids'])\nval_df['attention_mask'] = val_df['text'].apply(lambda x: preprocess_text(x)['attention_mask'])\n\ntest_df['input_ids'] = test_df['text'].apply(lambda x: preprocess_text(x)['input_ids'])\ntest_df['attention_mask'] = test_df['text'].apply(lambda x: preprocess_text(x)['attention_mask'])\n\n# convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_df['label'].values)\nval_labels = torch.tensor(val_df['label'].values)\ntest_labels = torch.tensor(test_df['label'].values)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:39:25.956784Z","iopub.execute_input":"2024-04-03T02:39:25.957066Z","iopub.status.idle":"2024-04-03T02:50:57.671768Z","shell.execute_reply.started":"2024-04-03T02:39:25.957031Z","shell.execute_reply":"2024-04-03T02:50:57.669708Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# custom PyTorch Dataset\nclass EmotionsDataset(Dataset):\n    def __init__(self, input_ids, attention_masks, labels):\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_masks[idx],\n            'labels': self.labels[idx]\n        }\n\ntrain_dataset = EmotionsDataset(train_df['input_ids'].tolist(), train_df['attention_mask'].tolist(), train_labels)\nval_dataset = EmotionsDataset(val_df['input_ids'].tolist(), val_df['attention_mask'].tolist(), val_labels)\ntest_dataset = EmotionsDataset(test_df['input_ids'].tolist(), test_df['attention_mask'].tolist(), test_labels)\n\n# create dataloaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:50:57.673125Z","iopub.execute_input":"2024-04-03T02:50:57.673428Z","iopub.status.idle":"2024-04-03T02:50:57.700869Z","shell.execute_reply.started":"2024-04-03T02:50:57.673400Z","shell.execute_reply":"2024-04-03T02:50:57.700046Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:50:57.701929Z","iopub.execute_input":"2024-04-03T02:50:57.702231Z","iopub.status.idle":"2024-04-03T02:50:57.719386Z","shell.execute_reply.started":"2024-04-03T02:50:57.702206Z","shell.execute_reply":"2024-04-03T02:50:57.718463Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\nnum_classes = len(train_df['label'].unique())\n\nepochs = 5\n\ntotal_steps = len(train_loader) * epochs\n\n# pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n\ndropout_rate = 0.5\nmodel.dropout1 = nn.Dropout(dropout_rate)\n\n# Batch normalization\nbatch_norm = nn.BatchNorm1d(768)\n\nmodel.dropout2 = nn.Dropout(dropout_rate)\n\n# optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:50:57.720622Z","iopub.execute_input":"2024-04-03T02:50:57.721163Z","iopub.status.idle":"2024-04-03T02:51:01.345485Z","shell.execute_reply.started":"2024-04-03T02:50:57.721130Z","shell.execute_reply":"2024-04-03T02:51:01.344626Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d052bd20c304ed1ac1b62793fbc60d7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=6, bias=True)\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\nnum_epochs = 5 \n\nfor epoch in range(num_epochs):\n    model.train() \n    total_loss = 0.0 \n    \n    for batch in train_loader:\n        \n        inputs = batch['input_ids'].squeeze(1).to(device)\n        attention_masks = batch['attention_mask'].squeeze(1).to(device)\n        labels = batch['labels'].to(device)\n\n        # clear gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n\n        # compute loss\n        loss = outputs.loss\n\n        # backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        \n    # calc avg training loss\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}: Avg. Training Loss: {avg_train_loss:.4f}\")\n\n    # validation loop\n    model.eval() \n    val_loss = 0.0\n    num_val_batches = len(val_loader)\n    val_preds = [] \n    val_true_labels = [] \n\n    with torch.no_grad():\n        for val_batch in val_loader:\n            val_inputs = val_batch['input_ids'].squeeze(1).to(device)\n            val_attention_masks = val_batch['attention_mask'].squeeze(1).to(device)\n            val_true = val_batch['labels'].to(device)\n\n            val_outputs = model(val_inputs, attention_mask=val_attention_masks, labels=val_true) \n            val_loss += val_outputs.loss.item()\n\n            # store predictions\n            val_preds.extend(torch.argmax(val_outputs.logits, axis=1).cpu().numpy().tolist())\n            val_true_labels.extend(val_true.cpu().numpy().tolist())\n\n    avg_val_loss = val_loss / num_val_batches\n    print(f\"Epoch {epoch+1}/{num_epochs}: Avg. Validation Loss: {avg_val_loss:.4f}\")\n\n    # eval metrics\n    val_accuracy = accuracy_score(val_true_labels, val_preds)\n    val_f1 = f1_score(val_true_labels, val_preds, average='weighted')\n    print(f\"Epoch {epoch+1}/{num_epochs}: Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:51:01.346650Z","iopub.execute_input":"2024-04-03T02:51:01.346921Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5: Avg. Training Loss: 0.1285\nEpoch 1/5: Avg. Validation Loss: 0.0881\nEpoch 1/5: Validation Accuracy: 0.9416, Validation F1 Score: 0.9426\nEpoch 2/5: Avg. Training Loss: 0.0873\nEpoch 2/5: Avg. Validation Loss: 0.0857\nEpoch 2/5: Validation Accuracy: 0.9435, Validation F1 Score: 0.9415\nEpoch 3/5: Avg. Training Loss: 0.0822\nEpoch 3/5: Avg. Validation Loss: 0.0850\nEpoch 3/5: Validation Accuracy: 0.9435, Validation F1 Score: 0.9442\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()  \ntest_preds = [] \ntest_labels = []  \n\nwith torch.no_grad():\n    for test_batch in test_loader:\n        test_inputs = test_batch['input_ids'].squeeze(1).to(device)\n        test_attention_masks = test_batch['attention_mask'].squeeze(1).to(device)\n        test_true_labels = test_batch['labels'].to(device)\n\n        # forward pass test\n        test_outputs = model(test_inputs, attention_mask=test_attention_masks)\n        \n        # store predictions\n        test_preds.extend(torch.argmax(test_outputs.logits, axis=1).cpu().numpy().tolist())\n        test_labels.extend(test_true_labels.cpu().numpy().tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nconf_matrix = confusion_matrix(test_labels, test_preds)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nConfusion matrix - BERT:\")\nconf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(test_labels, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}